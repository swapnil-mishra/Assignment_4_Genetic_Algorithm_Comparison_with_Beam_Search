Question C (c) Discussion: GA vs Beam Search (Diversity, Determinism, and Suitability for Language Generation)
=====================================================================================

1. Which method found more diverse solutions?
-------------------------------------------
In our experiments, the Genetic Algorithm (GA) inherently explored a *population* of candidate knapsack solutions simultaneously. Even after convergence to the global optimum (value = 96), intermediate generations contained multiple high‑scoring but structurally different chromosomes (i.e., distinct subsets of items) due to crossover + mutation operations. Beam Search, in contrast, maintained only a fixed number (beam width) of partial or complete candidates at each depth. Although Beam Search can retain multiple branches, pruning aggressively favors those with the highest *optimistic bound*, causing rapid focus (exploitation) and less structural diversity overall.

Key intuition:
- GA: Diversity is a first-class property via population mechanics, probabilistic variation, and selection pressure tuning.
- Beam Search: Diversity is incidental and constrained by beam width; states with slightly weaker heuristic scores are discarded early, even if they could recombine into novel good solutions (recombination does not exist in beam search).

Therefore: The GA produced *more diverse* solutions across the run.

2. Which is more deterministic and why?
--------------------------------------
Beam Search (with a fixed tie-breaking rule and identical heuristic ordering) is **deterministic**: given the same problem, beam width, depth, and scoring/expansion functions, it will select the exact same sequence of states every run.

The Genetic Algorithm is **stochastic**: random initialization, selection tournaments, crossover points, and mutation events introduce randomness at multiple stages. Even with the same random seed you can reproduce a run, but changing the seed changes the trajectory.

Thus: Beam Search is more deterministic (when implemented without random tie-breaking). GA is intentionally non-deterministic to balance exploration and exploitation.

3. Which might be preferable for language generation tasks where diversity matters?
-------------------------------------------------------------------------------
For open‑ended generation tasks (e.g., creative language generation, paraphrasing, story continuation), **diversity is often crucial** (to avoid repetitive or overly safe outputs). A GA’s population-based search naturally provides multiple distinct hypotheses each generation. You can:
- Sample from current population.
- Maintain an *archive* of novel individuals (e.g., using minimal Hamming distance thresholds or novelty scores).
- Use multi‑objective formulations (e.g., maximize relevance + diversity simultaneously).

Beam Search, especially narrow beams, tends to produce closely related sequences (minor variations in late tokens). It often suffers from *mode collapse* in creative tasks—converging to generic high-probability outputs. While techniques like diverse beam search exist, they require explicit penalty mechanisms or disjoint grouping heuristics to encourage diversity.

Therefore: A GA‑style (or other evolutionary / population-based) approach is typically more favorable when **diversity matters strongly**, although hybrid strategies (e.g., evolve prompts or latent vectors + use a decoder) can combine strengths.

Sample Code: Measuring Diversity in GA vs Beam Search
----------------------------------------------------
Below is illustrative (non-production) code demonstrating how you could quantify diversity using pairwise Hamming distance for the GA population and compare it to the (much smaller) set of Beam Search candidates.

```python
from itertools import combinations
from statistics import mean

# Assume after a GA run you still have: ga.population (list of bit lists)
# and after a beam search you have the final beam list of states.

def hamming(a, b):
    return sum(x != y for x, y in zip(a, b))

def diversity(pop):
    if len(pop) < 2:
        return 0.0
    dists = [hamming(x, y) for x, y in combinations(pop, 2)]
    return mean(dists)

# Example (pseudo-code integration):
# GA diversity across generations (store snapshots if you modify GA to keep them)
# snapshots = [pop_gen0, pop_gen1, ..., pop_genN]
# ga_diversity_curve = [diversity(pop) for pop in snapshots]

# Beam search: you can capture 'beam' states each depth
# beam_snapshots = [beam_depth1, beam_depth2, ...]
# beam_diversity_curve = [diversity(beam) for beam in beam_snapshots]
```

Optional Enhancement: Adding Novelty Pressure to GA
--------------------------------------------------
You could modify the GA fitness to incorporate a novelty term encouraging dissimilarity from an archive:

```python
def novelty(chromosome, archive):
    if not archive:
        return 0.0
    # Reward distance to nearest neighbor (greater distance -> higher novelty)
    return min(sum(a != b for a, b in zip(chromosome, arch)) for arch in archive)

ALPHA = 0.1  # weight of novelty component

def composite_fitness(chromosome):
    base = fitness(chromosome)  # domain fitness (e.g., knapsack value)
    nov = novelty(chromosome, novelty_archive)
    return base + ALPHA * nov
```

Practical Considerations for Language Generation
------------------------------------------------
| Criterion              | GA / Evolutionary Methods                      | Beam Search                          |
|------------------------|-----------------------------------------------|---------------------------------------|
| Diversity (raw)        | High (population & mutation)                  | Low–Moderate (depends on width)      |
| Determinism            | Low (unless seed fixed)                       | High                                 |
| Compute Pattern        | Parallelizable (evaluate population)          | Sequential per depth                 |
| Fine-Grained Control   | Crossover/mutation rate adjustments           | Beam width / heuristic adjustments   |
| Typical Use in NLP     | Prompt / architecture / latent evolution      | Token-level decoding (standard)      |
| Avoiding Mode Collapse | Natural via population variance               | Requires extra heuristics            |

Conclusion
----------
- More Diverse: Genetic Algorithm.
- More Deterministic: Beam Search.
- Preferable for Diversity in Language Generation: Genetic Algorithm (or other population‑based / evolutionary methods), potentially hybridized with model-guided scoring or used to evolve prompts/latent seeds.


